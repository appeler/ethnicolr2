{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Analytics and Performance\n",
    "\n",
    "This notebook explores advanced features of ethnicolr2 including batch processing, performance optimization, and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import ethnicolr2 functions\n",
    "from ethnicolr2 import (\n",
    "    pred_fl_last_name,\n",
    "    pred_fl_full_name, \n",
    "    pred_census_last_name\n",
    ")\n",
    "\n",
    "print(\"Advanced analytics setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Let's test performance with a larger synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger synthetic dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Common surnames from different ethnic backgrounds\n",
    "surnames = [\n",
    "    'Smith', 'Johnson', 'Williams', 'Brown', 'Jones',  # Common American\n",
    "    'Garcia', 'Rodriguez', 'Martinez', 'Hernandez', 'Lopez',  # Hispanic\n",
    "    'Zhang', 'Wang', 'Li', 'Liu', 'Chen',  # Chinese\n",
    "    'Kim', 'Lee', 'Park', 'Choi', 'Jung',  # Korean\n",
    "    'Patel', 'Shah', 'Singh', 'Kumar', 'Sharma'  # South Asian\n",
    "]\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_samples = 1000\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(n_samples),\n",
    "    'last_name': np.random.choice(surnames, n_samples)\n",
    "})\n",
    "\n",
    "print(f\"Created dataset with {len(large_df)} rows\")\n",
    "print(f\"Unique surnames: {large_df['last_name'].nunique()}\")\n",
    "display(large_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different models\n",
    "models = {\n",
    "    'Census Last Name': lambda df: pred_census_last_name(df, lname_col='last_name'),\n",
    "    'Florida Last Name': lambda df: pred_fl_last_name(df, lname_col='last_name')\n",
    "}\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for model_name, model_func in models.items():\n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result_df = model_func(large_df.copy())\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    rows_per_second = len(large_df) / execution_time\n",
    "    \n",
    "    performance_results.append({\n",
    "        'Model': model_name,\n",
    "        'Rows': len(large_df),\n",
    "        'Time (s)': round(execution_time, 3),\n",
    "        'Rows/sec': round(rows_per_second, 1)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {execution_time:.3f}s ({rows_per_second:.1f} rows/sec)\")\n",
    "\n",
    "perf_df = pd.DataFrame(performance_results)\n",
    "print(\"\\nPerformance Summary:\")\n",
    "display(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "Analyze prediction confidence and uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "results = pred_fl_last_name(large_df.copy(), lname_col='last_name')\n",
    "\n",
    "# Extract max probabilities (confidence)\n",
    "max_probs = []\n",
    "for _, row in results.iterrows():\n",
    "    probs = row['probs']\n",
    "    max_prob = max(probs.values())\n",
    "    max_probs.append(max_prob)\n",
    "\n",
    "results['confidence'] = max_probs\n",
    "\n",
    "print(\"Confidence Statistics:\")\n",
    "print(f\"Mean confidence: {np.mean(max_probs):.3f}\")\n",
    "print(f\"Median confidence: {np.median(max_probs):.3f}\")\n",
    "print(f\"Min confidence: {np.min(max_probs):.3f}\")\n",
    "print(f\"Max confidence: {np.max(max_probs):.3f}\")\n",
    "\n",
    "# Confidence by prediction category\n",
    "confidence_by_pred = results.groupby('preds')['confidence'].agg(['mean', 'count'])\n",
    "confidence_by_pred.columns = ['avg_confidence', 'count']\n",
    "confidence_by_pred = confidence_by_pred.round(3)\n",
    "\n",
    "print(\"\\nConfidence by Prediction Category:\")\n",
    "display(confidence_by_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This advanced analytics notebook demonstrated:\n",
    "\n",
    "1. **Performance benchmarking** across different models\n",
    "2. **Statistical analysis** of prediction confidence\n",
    "3. **Large-scale processing** patterns\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- ethnicolr2 efficiently handles large datasets\n",
    "- Probability distributions provide valuable uncertainty information\n",
    "- Different models have different performance characteristics\n",
    "- Confidence scores help identify uncertain predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
